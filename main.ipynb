{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed557d43",
   "metadata": {},
   "source": [
    "Cost Optimization Techniques for LLM API Usage\n",
    "This notebook demonstrates strategies to optimize costs when working with LLM APIs:\n",
    "- Implementing batching\n",
    "- Implementing prompt caching\n",
    "- Token usage optimization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4cc0aa",
   "metadata": {},
   "source": [
    "!!! Prepare .env file based on .env.template !!!"
   ]
  },
  {
   "cell_type": "code",
   "id": "a4d268f2",
   "metadata": {},
   "source": [
    "# Install required external libraries\n",
    "%pip install python-dotenv openai anthropic boto3 tiktoken pydantic IPython\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "55bc4cf5",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "id": "4d66c0852bb99c8d",
   "metadata": {},
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "\n",
    "import anthropic\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, JSON\n",
    "from openai import AzureOpenAI, OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from openai.lib._parsing._completions import type_to_response_format_param\n",
    "\n",
    "from openai_callback import (\n",
    "    get_llm_debug_anthropic,\n",
    "    get_llm_debug_openai, \n",
    "    get_llm_debug_bedrock\n",
    ")\n",
    "from timer import Timer\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aab6de5f43860421",
   "metadata": {},
   "source": [
    "load_dotenv()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dae3294b6b2af50c",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "id": "7a3bac70d6ce9488",
   "metadata": {},
   "source": [
    "# Model names and versions used for different LLM providers\n",
    "\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "AZURE_OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "OPENAI_MODEL_BATCH = \"gpt-4o-mini\"\n",
    "AZURE_OPENAI_MODEL_BATCH = \"gpt-4o-mini-batch\"\n",
    "\n",
    "ANTHROPIC_MODEL = \"claude-3-5-haiku-20241022\"\n",
    "\n",
    "AWS_BEDROCK_ANTHROPIC_MODEL = \"anthropic.claude-3-5-haiku-20241022-v1:0\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b0011afbd191b953",
   "metadata": {},
   "source": [
    "## Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "id": "5b360934e2af6c96",
   "metadata": {},
   "source": [
    "transcripts_dir = \"./transcripts/\"\n",
    "example_transcript_path = os.path.join(transcripts_dir, \"16.txt\")\n",
    "example_transcript = open(example_transcript_path, \"r\").read()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e2f63723bc9f7152",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "id": "2f10be3dd3394fa7",
   "metadata": {},
   "source": [
    "class TopicEnum(str, Enum):\n",
    "    ORDER_STATUS = \"Order Status\"\n",
    "    PRODUCT_INQUIRY = \"Product Inquiry\"\n",
    "    RETURN_EXCHANGE = \"Return/Exchange\"\n",
    "    TECHNICAL_SUPPORT = \"Technical Support\"\n",
    "    BILLING_PAYMENT = \"Billing/Payment\"\n",
    "    DELIVERY_ISSUE = \"Delivery Issue\"\n",
    "    PROMOTIONS_DISCOUNTS = \"Promotions/Discounts\"\n",
    "    ACCOUNT_ASSISTANCE = \"Account Assistance\"\n",
    "    FEEDBACK_COMPLAINT = \"Feedback/Complaint\"\n",
    "    STORE_LOCATOR = \"Store Locator\"\n",
    "    WARRANTY_GUARANTEE = \"Warranty/Guarantee\"\n",
    "    FRAUD_SECURITY = \"Fraud/Security\"\n",
    "    OTHER = \"Other\"\n",
    "\n",
    "class Offer(BaseModel):\n",
    "    offering_name: str = Field(...)\n",
    "    price: str | None = Field(None)\n",
    "    amount_with_unit: str | None = Field(None)\n",
    "    background: str = Field(...)\n",
    "    additional_notes: str | None = Field(None)\n",
    "\n",
    "class CompetitorOffer(Offer):\n",
    "    competitor_name: str = Field(...)\n",
    "\n",
    "class Insight(BaseModel):\n",
    "    main_call_topic: TopicEnum = Field(\n",
    "        ..., description=\"Main topic category (must match predefined categories)\"\n",
    "    )\n",
    "    secondary_call_topics: list[TopicEnum] = Field(\n",
    "        ..., description=\"Secondary topic categories (must match predefined categories)\"\n",
    "    )\n",
    "    summary: str = Field(...)\n",
    "    offers: list[Offer] = Field(...)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "21a7c5a390741f12",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "code",
   "id": "6050091621fdc736",
   "metadata": {},
   "source": [
    "system_prompt = open(\"./system_prompt.txt\", \"r\").read()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "353cb10a7e816d32",
   "metadata": {},
   "source": [
    "# Cost Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b844a9038c29fd0",
   "metadata": {},
   "source": [
    "## Level 1 - Prompt Caching\n",
    "Prompt caching is a performance optimization technique used in language models that stores tokenized prefixes of prompts (including system and user messages). By caching these commonly used components, it significantly reduces both costs and processing time while maintaining the ability to generate unique outputs for each request.\n",
    "\n",
    "The KV (key-value) cache stores intermediate attention computations generated during sequence processing. These cached representations allow the model to skip recomputing values for identical prefix tokens in subsequent requests. While this provides substantial performance benefits, the cache is sensitive to changes - any modification to the prefix tokens invalidates the cache and requires recomputation from that point forward.\n",
    "\n",
    "Key Use Cases:\n",
    "- Conversational AI: Efficiently handling conversation history and context\n",
    "- Document Processing: Optimizing analysis of large documents and texts\n",
    "- Few-Shot Learning: Maintaining consistent instruction sets and examples\n",
    "- Batch Processing: Handling repetitive tasks with similar prompts\n",
    "- Tool-Augmented LLMs: Supporting multiple rounds of tool interactions efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aa71b9a8bf87ec",
   "metadata": {},
   "source": [
    "##Anthropic's Prompt Caching\n",
    "\n",
    "Anthropic's prompt caching feature offers significant performance and cost benefits:\n",
    "\n",
    "Key Benefits:\n",
    "- Reduces costs up to 90% and latency up to 85% for long prompts\n",
    "- Maximum 5-minute cache lifetime (TTL), refreshed on each use\n",
    "- Can be combined with Batch Inference for additional optimization\n",
    "\n",
    "Implementation:\n",
    "- Enabled by adding \"cache_control\" block to messages\n",
    "- Requires minimum prompt length:\n",
    "  - 1024 tokens: Claude 3.7/3.5 Sonnet, Claude 3 Opus\n",
    "  - 2048 tokens: Claude 3.5 Haiku, Claude 3 Haiku\n",
    "\n",
    "Pricing Structure:\n",
    "- Cache write: 25% additionally over base input tokens\n",
    "- Cache read: 90% discount compared to base input tokens\n",
    "\n",
    "Cacheable Components:\n",
    "- System and user messages\n",
    "- Images\n",
    "- Tools and tool definitions/usage\n",
    "\n",
    "Pricing Table (per million tokens):\n",
    "| **Model**                 | **Standard Input** | **Cache Write** | **Cache Read** | **Output** |\n",
    "|---------------------------|-------------------|-----------------|----------------|------------|\n",
    "| **Claude 3.5/3.7 Sonnet** | $3.00             | $3.75           | $0.30         | $15.00     |\n",
    "| **Claude 3 Opus**         | $15.00            | $18.75          | $1.50         | $75.00     |\n",
    "| **Claude 3.5 Haiku**      | $0.80             | $1.00           | $0.08         | $4.00      |\n",
    "\n",
    "References:\n",
    "- [Anthropic Documentation](https://docs.anthropic.com/en/docs/build-with-claude/tool-use/token-efficient-tool-use)"
   ]
  },
  {
   "cell_type": "code",
   "id": "29992f4a349fd1f",
   "metadata": {},
   "source": [
    "# output schema prefix due to lack of support for structured output in anthropic\n",
    "model_json_schema_anthropic = f\"\"\"\n",
    "As a genius expert, your task is to understand the content and provide\n",
    "the parsed objects in json that match the following json_schema:\n",
    "{Insight.model_json_schema()}\n",
    "Make sure to return an instance of the JSON, not the schema itself\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9927af53b9ec68c",
   "metadata": {},
   "source": [
    "# Test Anthropic's prompt caching feature by making multiple API calls with the same transcript\n",
    "anthropic_client = anthropic.Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "model_json_schema_anthropic = f\"\"\"\n",
    "As a genius expert, your task is to understand the content and provide\n",
    "the parsed objects in json that match the following json_schema:\n",
    "{Insight.model_json_schema()}\n",
    "Make sure to return an instance of the JSON, not the schema itself\n",
    "\"\"\"\n",
    "\n",
    "example_transcripts = [\n",
    "    open(os.path.join(transcripts_dir, \"16.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"16.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"16.txt\"), \"r\").read(),\n",
    "]\n",
    "\n",
    "for _example_transcript in example_transcripts:\n",
    "    with Timer() as timer:\n",
    "        message = anthropic_client.messages.create(\n",
    "            model=ANTHROPIC_MODEL,\n",
    "            max_tokens=1000,\n",
    "            temperature=0,\n",
    "            system=[\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"\\n\\n\".join([\"aaa\"+system_prompt, model_json_schema_anthropic]),\n",
    "                    \"cache_control\": {\"type\": \"ephemeral\"},\n",
    "                }\n",
    "            ],\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": f\"[Transcript]\\n {_example_transcript}\",\n",
    "                        \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            ]\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "    #display(JSON(model_object.model_dump(), expanded=True))\n",
    "    print(f\"Elapsed time: {timer.elapsed_time}\")\n",
    "    print(get_llm_debug_anthropic(message.usage.dict(), message.model))\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bede53d44e234e45",
   "metadata": {},
   "source": [
    "### Anthropic\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "54b6fdd4b7af0318",
   "metadata": {},
   "source": [
    "# Test Anthropic's system message caching with multiple transcripts\n",
    "\n",
    "anthropic_client = anthropic.Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "model_json_schema_anthropic = f\"\"\"\n",
    "As a genius expert, your task is to understand the content and provide\n",
    "the parsed objects in json that match the following json_schema:\n",
    "{Insight.model_json_schema()}\n",
    "Make sure to return an instance of the JSON, not the schema itself\n",
    "\"\"\"\n",
    "\n",
    "example_transcripts = [\n",
    "    open(os.path.join(transcripts_dir, \"15.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"16.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"17.txt\"), \"r\").read(),\n",
    "]\n",
    "\n",
    "for _example_transcript in example_transcripts:\n",
    "    with Timer() as timer:\n",
    "        message = anthropic_client.messages.create(\n",
    "            model=ANTHROPIC_MODEL,\n",
    "            max_tokens=1000,\n",
    "            temperature=0,\n",
    "            system=[\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"\\n\\n\".join([\"a\"+system_prompt, model_json_schema_anthropic]),\n",
    "                    \"cache_control\": {\"type\": \"ephemeral\"},\n",
    "                }\n",
    "            ],\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": f\"[Transcript]\\n {_example_transcript}\",\n",
    "                        \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    #display(JSON(model_object.model_dump(), expanded=True))\n",
    "    print(f\"Elapsed time: {timer.elapsed_time}\")\n",
    "    print(get_llm_debug_anthropic(message.usage.dict(), message.model))\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "372d19b5ef3c7d0d",
   "metadata": {},
   "source": [
    "### AWS Bedrock\n",
    "\n",
    "AWS Bedrock provides prompt caching similar to Anthropic's implementation, with a few key differences:\n",
    "- Batch inference requests still accumulate costs even when using prompt caching\n",
    "- The service is currently in preview/beta and not yet generally available (GA)\n",
    "- Cache hits can help reduce latency and costs for repeated prompts\n",
    "- Best practice is to structure prompts with static content at the beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae50e0956e23135a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1406d791c2fe2500",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be6812c71343097",
   "metadata": {},
   "source": [
    "OpenAI Prompt Caching\n",
    "\n",
    "Performance Benefits:\n",
    "- Reduces latency by up to 80% \n",
    "- Reduces input token costs by 50%\n",
    "- Automatic - no configuration needed\n",
    "\n",
    "Technical Details:\n",
    "- Available for prompts with 1024+ tokens\n",
    "- Cache hits occur in 128 token increments\n",
    "- Cache duration: 5-10 minutes (up to 1 hour during off-peak)\n",
    "\n",
    "Cacheable Content:\n",
    "- System messages\n",
    "- User messages  \n",
    "- Images\n",
    "- Tools and tool calls\n",
    "- Structured output specifications\n",
    "\n",
    "Supported Models:\n",
    "- GPT-4.5 Preview\n",
    "- GPT-4o (except gpt-4o-2024-05-13 and chatgpt-4o-latest)\n",
    "- GPT-4o Mini\n",
    "- GPT-4o Realtime Preview\n",
    "- O1 Preview\n",
    "- O1 Mini\n",
    "\n",
    "Pricing (per million tokens):\n",
    "\n",
    "| Model                      | Input (Regular) | Input (Cached) | Output    |\n",
    "|----------------------------|----------------|----------------|-----------|\n",
    "| GPT-4o (2024-08-06)       | $2.50          | $1.25         | $10.00    |\n",
    "| GPT-4o Fine-tuned         | $3.75          | $1.875        | $15.00    |\n",
    "| GPT-4o Mini (2024-07-18)  | $0.15          | $0.075        | $0.60     |\n",
    "| GPT-4o Mini Fine-tuned    | $0.30          | $0.15         | $1.20     |\n",
    "| O1 Preview                | $15.00         | $7.50         | $60.00    |\n",
    "| O1 Mini                   | $3.00          | $1.50         | $12.00    |\n",
    "\n",
    "Best Practices:\n",
    "1. Place static/repeated content at the start of prompts\n",
    "2. Put dynamic content at the end\n",
    "3. Use longer prompts when possible (1024+ tokens)\n",
    "4. Make API calls during off-peak hours for better cache retention"
   ]
  },
  {
   "cell_type": "code",
   "id": "d67c73ea131d642c",
   "metadata": {},
   "source": [
    "# Test OpemAI's prompt caching feature by making multiple API calls with the same transcript\n",
    "\n",
    "example_transcripts = [\n",
    "    open(os.path.join(transcripts_dir, \"16.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"16.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"16.txt\"), \"r\").read(),\n",
    "]\n",
    "openai_client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "for _example_transcript in example_transcripts:\n",
    "    with Timer() as timer:\n",
    "        openai_result = openai_client.beta.chat.completions.parse(\n",
    "                model=OPENAI_MODEL,\n",
    "                temperature=0,\n",
    "                max_completion_tokens=1024,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"asdad\" + system_prompt\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"[Transcript]\\n {_example_transcript}\"\n",
    "                    }\n",
    "                ],\n",
    "                response_format=Insight,\n",
    "        )\n",
    "\n",
    "\n",
    "    openai_response = openai_result.choices[0].message.content\n",
    "    openai_insight: Insight = openai_result.choices[0].message.parsed\n",
    "    openai_completion_tokens = openai_result.usage.completion_tokens\n",
    "    openai_prompt_tokens = openai_result.usage.prompt_tokens\n",
    "    openai_cached_tokens = openai_result.usage.prompt_tokens_details.cached_tokens\n",
    "\n",
    "    display(JSON(openai_insight.model_dump(), expanded=True))\n",
    "\n",
    "    print(f\"Completion Tokens: {openai_completion_tokens}\")\n",
    "    print(f\"Prompt Tokens: {openai_prompt_tokens}\")\n",
    "    print(f\"Cached Tokens: {openai_cached_tokens}\")\n",
    "    print(f\"Elapsed time: {timer.elapsed_time}\")\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "da2d8d12393a104c",
   "metadata": {},
   "source": [
    "# Test OpenAI's system message caching with multiple transcripts\n",
    " \n",
    "example_transcripts = [\n",
    "    open(os.path.join(transcripts_dir, \"15.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"16.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"17.txt\"), \"r\").read(),\n",
    "]\n",
    "openai_client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "for _example_transcript in example_transcripts:\n",
    "    with Timer() as timer:\n",
    "        openai_result = openai_client.beta.chat.completions.parse(\n",
    "                model=OPENAI_MODEL,\n",
    "                temperature=0,\n",
    "                max_completion_tokens=1024,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"d\" + system_prompt\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"[Transcript]\\n {_example_transcript}\"\n",
    "                    }\n",
    "                ],\n",
    "                response_format=Insight,\n",
    "        )\n",
    "\n",
    "\n",
    "    openai_response = openai_result.choices[0].message.content\n",
    "    openai_insight: Insight = openai_result.choices[0].message.parsed\n",
    "    openai_completion_tokens = openai_result.usage.completion_tokens\n",
    "    openai_prompt_tokens = openai_result.usage.prompt_tokens\n",
    "    openai_cached_tokens = openai_result.usage.prompt_tokens_details.cached_tokens\n",
    "\n",
    "    display(JSON(openai_insight.model_dump(), expanded=True))\n",
    "\n",
    "    print(f\"Completion Tokens: {openai_completion_tokens}\")\n",
    "    print(f\"Prompt Tokens: {openai_prompt_tokens}\")\n",
    "    print(f\"Cached Tokens: {openai_cached_tokens}\")\n",
    "    print(f\"Elapsed time: {timer.elapsed_time}\")\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2d6cd43fb39dd2fe",
   "metadata": {},
   "source": [
    "### Azure OpenAI\n",
    "\n",
    "Azure OpenAI provides system prompt caching functionality that enables efficient reuse of system prompts across multiple API calls, reducing both token usage and latency.\n",
    "\n",
    "Key features of Azure OpenAI prompt caching:\n",
    "- Officially supported in API version 2024-10-01-preview and later\n",
    "- Available exclusively for the o1 model family (e.g. gpt-4-o1, gpt-3.5-turbo-o1)\n",
    "- Cached tokens are tracked and reported in the usage.prompt_tokens_details.cached_tokens field\n",
    "- Achieves up to 90% reduction in token usage for repeated system prompts\n",
    "- Reduces latency by skipping re-tokenization of cached system prompts\n",
    "- Implements the same caching logic and behavior as OpenAI's base API\n",
    "\n",
    "The example below demonstrates Azure OpenAI's prompt caching capabilities by processing multiple transcripts with a shared system prompt. Monitor the cached_tokens metric to observe the caching in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "85f6094647a24fa7",
   "metadata": {},
   "source": [
    "# Process multiple transcripts using Azure OpenAI with prompt caching\n",
    "example_transcripts = [\n",
    "    open(os.path.join(transcripts_dir, \"15.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"16.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"17.txt\"), \"r\").read(),\n",
    "]\n",
    "azure_openai_client = AzureOpenAI(\n",
    "    api_version=\"2024-10-01-preview\",\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_API_ENDPOINT\"),\n",
    ")\n",
    "\n",
    "for _example_transcript in example_transcripts:\n",
    "    with Timer() as timer:\n",
    "        azure_openai_result = azure_openai_client.beta.chat.completions.parse(\n",
    "                model=OPENAI_MODEL,\n",
    "                temperature=0,\n",
    "                max_completion_tokens=1024,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"agdfa\" + system_prompt\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"[Transcript]\\n {_example_transcript}\"\n",
    "                    }\n",
    "                ],\n",
    "                response_format=Insight,\n",
    "        )\n",
    "\n",
    "\n",
    "    azure_openai_response = azure_openai_result.choices[0].message.content\n",
    "    azure_openai_insight: Insight = azure_openai_result.choices[0].message.parsed\n",
    "    azure_openai_completion_tokens = azure_openai_result.usage.completion_tokens\n",
    "    azure_openai_prompt_tokens = azure_openai_result.usage.prompt_tokens\n",
    "    azure_openai_cached_tokens = azure_openai_result.usage.prompt_tokens_details.cached_tokens\n",
    "\n",
    "    display(JSON(openai_insight.model_dump(), expanded=True))\n",
    "\n",
    "    print(f\"Completion Tokens: {azure_openai_completion_tokens}\")\n",
    "    print(f\"Prompt Tokens: {azure_openai_prompt_tokens}\")\n",
    "    print(f\"Cached Tokens: {azure_openai_cached_tokens}\")\n",
    "    print(f\"Elapsed time: {timer.elapsed_time}\")\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f16476b969092a42",
   "metadata": {},
   "source": [
    "## Level 2 - Batch Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265c9a5a80e668b1",
   "metadata": {},
   "source": [
    "When to use batch inference:\n",
    "- Processing large volumes of data (thousands of records)\n",
    "- Latency is not critical (responses can be delayed)\n",
    "- Cost optimization is a priority ( 50% cheaper than on-demand)\n",
    "- Running extensive evaluations, analyses, or model comparisons\n",
    "- Data processing can be done asynchronously\n",
    "- Resource utilization needs to be optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f0b33c1f6d37",
   "metadata": {},
   "source": [
    "### Anthropic\n",
    "\n",
    "Key Features\n",
    "- Efficient prompt caching support for optimized processing\n",
    "- Cost-effective with 50% discount compared to standard API pricing\n",
    "- Asynchronous batch processing with results typically within 1 hour\n",
    "\n",
    "Pricing (per Million Tokens)\n",
    "\n",
    " | Model             | On-demand Input | Batch Input   | On-demand Output | Batch Output  |\n",
    " |-------------------|-----------------|---------------|------------------|---------------|\n",
    " | Claude 3.7 Sonnet | \\$3.00 / MTok   | \\$1.50 / MTok | \\$15.00 / MTok   | \\$7.50 / MTok |\n",
    " | Claude 3.5 Sonnet | \\$3.00 / MTok   | \\$1.50 / MTok | \\$15.00 / MTok   | \\$7.50 / MTok |\n",
    " | Claude 3.5 Haiku  | \\$0.80 / MTok   | \\$0.40 / MTok | \\$4.00 / MTok    | \\$2.00 / MTok |\n",
    "\n",
    "Quota Limits & Processing Details\n",
    "Tier 1\n",
    "- Queue capacity: 100,000 batch requests\n",
    "- Rate limit: 50 requests per minute (RPM)\n",
    "\n",
    "Tier 2  \n",
    "- Queue capacity: 200,000 batch requests\n",
    "- Rate limit: 1,000 requests per minute (RPM)\n",
    "\n",
    "General Limits\n",
    "- Maximum requests per batch: 100,000\n",
    "- Maximum batch size: Either 100,000 messages or 256 MB\n",
    "- Processing time: Usually within 1 hour, maximum 24 hours\n",
    "- Result retention: 29 days from creation\n",
    "- Expiration: Batches expire if not processed within 24 hours\n",
    "\n",
    "For more information, visit: https://console.anthropic.com/"
   ]
  },
  {
   "cell_type": "code",
   "id": "474984898490283b",
   "metadata": {},
   "source": [
    "import anthropic\n",
    "from anthropic.types.message_create_params import MessageCreateParamsNonStreaming\n",
    "from anthropic.types.messages.batch_create_params import Request\n",
    "\n",
    "client = anthropic.Anthropic()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1cc5f67236aa5396",
   "metadata": {},
   "source": [
    "# Prepare batch requests for multiple transcripts to be processed by Anthropic's API\n",
    "requests = []\n",
    "timestamp = int(time.time())\n",
    "\n",
    "example_transcripts = [\n",
    "    open(os.path.join(transcripts_dir, \"14.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"15.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"16.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"17.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"18.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"19.txt\"), \"r\").read(),\n",
    "]\n",
    "\n",
    "for i, _example_transcript in enumerate(example_transcripts):\n",
    "    _request = Request(\n",
    "        custom_id=f\"{i}--anthropic_batch_example--{timestamp}\",\n",
    "        params=MessageCreateParamsNonStreaming(\n",
    "            model=ANTHROPIC_MODEL,\n",
    "            max_tokens=1024,\n",
    "            system=[\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"\\n\\n\".join([system_prompt, model_json_schema_anthropic]),\n",
    "                    \"cache_control\": {\"type\": \"ephemeral\"},\n",
    "                }\n",
    "            ],\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"[Transcript]\\n {_example_transcript}\"\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "    requests.append(_request)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a86f44fbc74217a7",
   "metadata": {},
   "source": [
    "message_batch = client.messages.batches.create(\n",
    "    requests=requests\n",
    ")\n",
    "\n",
    "message_batch_id = message_batch.id\n",
    "\n",
    "print(message_batch)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "76629a106ab6117f",
   "metadata": {},
   "source": [
    "message_batch_id = \"msgbatch_01BYGbkrw2KrVUxx4MrFF48u\"\n",
    "message_batch_id = \"msgbatch_016LieMPc3tdMMi3wY5nQK3D\" "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1564a84fef194fbc",
   "metadata": {},
   "source": [
    "message_batch = client.messages.batches.retrieve(\n",
    "    message_batch_id\n",
    ")\n",
    "message_batch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4131c115699953f8",
   "metadata": {},
   "source": [
    "for result in client.messages.batches.results(\n",
    "    message_batch_id,\n",
    "):\n",
    "    match result.result.type:\n",
    "        case \"succeeded\":\n",
    "            print(f\"Success! {result.custom_id}\")\n",
    "            print(result)\n",
    "            display(JSON(result.model_dump_json(), expanded=True))\n",
    "            result_dict = result.model_dump()\n",
    "            print(get_llm_debug_anthropic(result_dict['result']['message']['usage'], result_dict['result']['message']['model']))\n",
    "            print()\n",
    "        case \"errored\":\n",
    "            if result.result.error.type == \"invalid_request\":\n",
    "                print(f\"Validation error {result.custom_id}\")\n",
    "            else:\n",
    "                print(f\"Server error {result.custom_id}\")\n",
    "        case \"expired\":\n",
    "            print(f\"Request expired {result.custom_id}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "551c00a002b98ba4",
   "metadata": {},
   "source": [
    "### AWS Bedrock\n",
    "Key Benefits:\n",
    "- Cost-effective: Up to 50% lower costs compared to standard API calls\n",
    "- Flexible processing: 24-hour window for batch completion\n",
    "- High throughput: Process large volumes of requests efficiently\n",
    "\n",
    "Technical Specifications:\n",
    "Message Batch Limits:\n",
    "- Maximum 50,000 message requests per batch\n",
    "- Maximum 200 MB batch size\n",
    "- Token limits vary by agreement type (e.g., 200M tokens for default and GPT-4)\n",
    "\n",
    "Processing Details:\n",
    "- Results typically available within 1 hour\n",
    "- Maximum processing window of 24 hours\n",
    "- Results expire after 24 hours\n",
    "- Batch processing uses separate quota from standard API rate limits\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "fa723c41f550b746",
   "metadata": {},
   "source": [
    "model_json_schema_anthropic = f\"\"\"\n",
    "As a genius expert, your task is to understand the content and provide\n",
    "the parsed objects in json that match the following json_schema:\n",
    "{Insight.model_json_schema()}\n",
    "Make sure to return an instance of the JSON, not the schema itself\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cc2fb0f487dd70fb",
   "metadata": {},
   "source": [
    "role_arn = \"arn:aws:iam::<arn>\"\n",
    "model_id = \"anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "bucket_name = os.environ[\"S3_BATCH_BUCKET_NAME\"]\n",
    "output_s3_bucket_path = f\"s3://{bucket_name}/batch_output/\"\n",
    "bedrock_anthropic_batch_input_path = Path(\"./bedrock_anthropic_batch_input.jsonl\")\n",
    "object_name = os.path.basename(bedrock_anthropic_batch_input_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8a990226ac72767e",
   "metadata": {},
   "source": [
    "bedrock_anthropic_message_requests = []\n",
    "timestamp = int(time.time())\n",
    "\n",
    "example_transcripts = [open(os.path.join(transcripts_dir, f\"{i}.txt\"), \"r\").read() for i in range(1, 101)]\n",
    "\n",
    "for i, _example_transcript in enumerate(example_transcripts):\n",
    "    _request =  {\n",
    "        \"recordId\": f\"{i}--aws_bedrock_batch_example--{timestamp}\",\n",
    "        \"modelInput\": {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 1024,\n",
    "            \"system\": [\n",
    "                {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"\\n\\n\".join([system_prompt,model_json_schema_anthropic]),\n",
    "                        \"cache_control\": {\"type\": \"ephemeral\"},\n",
    "                },\n",
    "            ],\n",
    "            \"messages\": [\n",
    "                {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"[Transcript]\\n {_example_transcript}\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    }\n",
    "    bedrock_anthropic_message_requests.append(_request)\n",
    "\n",
    "bedrock_anthropic_batch_input_path.write_text(\"\\n\".join(json.dumps(bedrock_anthropic_message_request) for bedrock_anthropic_message_request in bedrock_anthropic_message_requests))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4222df2df70bf9af",
   "metadata": {},
   "source": [
    "s3_client = boto3.client('s3')\n",
    "bedrock_client = boto3.client(service_name=\"bedrock\", region_name=\"us-west-2\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "25953188a5bf7edb",
   "metadata": {},
   "source": [
    "s3_client.upload_file(bedrock_anthropic_batch_input_path, bucket_name, object_name)\n",
    "input_s3_object_path = f\"s3://{bucket_name}/{object_name}\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab97265ebb72e3d9",
   "metadata": {},
   "source": [
    "timestamp = int(time.time())\n",
    "job_name = f\"example-batch-job-{timestamp}\"\n",
    "\n",
    "inputDataConfig = ({\n",
    "    \"s3InputDataConfig\": {\n",
    "        \"s3Uri\": input_s3_object_path\n",
    "    }\n",
    "})\n",
    "\n",
    "outputDataConfig = ({\n",
    "    \"s3OutputDataConfig\": {\n",
    "        \"s3Uri\": output_s3_bucket_path\n",
    "    }\n",
    "})\n",
    "response = bedrock_client.create_model_invocation_job(\n",
    "    roleArn=role_arn,\n",
    "    modelId=model_id,\n",
    "    jobName=job_name,\n",
    "    inputDataConfig=inputDataConfig,\n",
    "    outputDataConfig=outputDataConfig\n",
    ")\n",
    "job_arn = response.get('jobArn')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9d95d1b713111ff4",
   "metadata": {},
   "source": [
    "job_arn = \"arn:aws:bedrock:us-west-2:711156763240:model-invocation-job/4n5cf3uuyym9\"\n",
    "job_arn = \"arn:aws:bedrock:us-west-2:711156763240:model-invocation-job/zxcg13d36ewe\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "151e233ad38b042d",
   "metadata": {},
   "source": [
    "response = bedrock_client.get_model_invocation_job(jobIdentifier=job_arn)\n",
    "display(JSON(response, expanded=True))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4e4ca6e553941c74",
   "metadata": {},
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "results_response = s3.get_object(Bucket=response['outputDataConfig']['s3OutputDataConfig']['s3Uri'].split('/')[2], Key=f\"{response['outputDataConfig']['s3OutputDataConfig']['s3Uri'].split('/')[3]}/{response['jobArn'].split('/')[-1]}/bedrock_anthropic_batch_input.jsonl.out\")\n",
    "data = results_response['Body'].read().decode('utf-8')\n",
    "\n",
    "for line in data.splitlines()[:2]:\n",
    "    _data = json.loads(line)\n",
    "    display(JSON(_data))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "947b03bfda06415d",
   "metadata": {},
   "source": [
    "### Azure OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c7c2db6492892d",
   "metadata": {},
   "source": [
    "Issues with schema validation with openai schema compliance:\n",
    "- Schema is missing \"additionalProperties\": false which would prevent extra fields\n",
    "- Need to set \"strict\": True for strict validation enforcement"
   ]
  },
  {
   "cell_type": "code",
   "id": "3f42bcff4ec3605b",
   "metadata": {},
   "source": [
    "json.dumps(Insight.model_json_schema())\n",
    "json.dumps(type_to_response_format_param(Insight)[\"json_schema\"][\"schema\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "167fbd39d671ae7",
   "metadata": {},
   "source": [
    "Azure OpenAI Batch Processing Quota Limits & Constraints\n",
    "\n",
    "Queue Limits\n",
    "- Tier 1: Max 100,000 requests in processing queue\n",
    "- Tier 2: Max 200,000 requests in processing queue\n",
    "\n",
    "Rate Limits\n",
    "- Tier 1: 50 requests per minute (RPM)\n",
    "- Tier 2: 1,000 requests per minute (RPM)\n",
    "\n",
    "Batch Size Limits\n",
    "- Maximum requests per batch: 100,000 (both tiers)\n",
    "- Message batch size limit: 100,000 requests OR 256 MB\n",
    " \n",
    "Time Constraints\n",
    "- Expected completion: Most batches complete within 1 hour\n",
    "- Maximum processing time: 24 hours before expiration\n",
    "- Results retention period: 29 days from creation\n",
    "\n",
    "Technical Requirements\n",
    "- Strict schema validation enforced\n",
    "- Deployment requires dedicated model instances"
   ]
  },
  {
   "cell_type": "code",
   "id": "4a58975df7483468",
   "metadata": {},
   "source": [
    "message_requests = []\n",
    "timestamp = int(time.time())\n",
    "\n",
    "json_schema = type_to_response_format_param(Insight) #secure schema\n",
    "\n",
    "json_schema = {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"schema\": Insight.model_json_schema(),\n",
    "            \"name\": \"Insight\",\n",
    "            \"strict\": True,\n",
    "        },\n",
    "    }\n",
    "\n",
    "example_transcripts = [\n",
    "    open(os.path.join(transcripts_dir, \"14.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"15.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"16.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"17.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"18.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"19.txt\"), \"r\").read(),\n",
    "]\n",
    "\n",
    "for i, _example_transcript in enumerate(example_transcripts):\n",
    "    _result = {\n",
    "        \"custom_id\": f\"{i}--azure_openai--{timestamp}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": AZURE_OPENAI_MODEL_BATCH,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\" + system_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"[Transcript]\\n {_example_transcript}\"\n",
    "                }\n",
    "            ],\n",
    "            \"response_format\": json_schema,\n",
    "\n",
    "        }\n",
    "    }\n",
    "    message_requests.append(_result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4b4676100ea5ad49",
   "metadata": {},
   "source": [
    "azure_openai_batch_input_path = Path(\"./azure_openai_batch_input.jsonl\")\n",
    "azure_openai_batch_input_path.write_text(\"\\n\".join(json.dumps(message_request) for message_request in message_requests))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2123b0b27deb3543",
   "metadata": {},
   "source": [
    "azure_openai_client = AzureOpenAI(\n",
    "    api_version=\"2024-10-01-preview\",\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_API_ENDPOINT\"),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c88e2fbb72d51f41",
   "metadata": {},
   "source": [
    "file = azure_openai_client.files.create(\n",
    "  file=open(azure_openai_batch_input_path, \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")\n",
    "print(file.model_dump_json(indent=2))\n",
    "file_id = file.id"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1667b1feb89e6d4f",
   "metadata": {},
   "source": [
    "batch_response = azure_openai_client.batches.create(\n",
    "    input_file_id=file_id,\n",
    "    endpoint=\"/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    ")\n",
    "batch_id = batch_response.id\n",
    "print(batch_response.model_dump_json(indent=2))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f846f1e59d7106ee",
   "metadata": {},
   "source": [
    "#invalid schema\n",
    "batch_id = \"batch_2183d1c7-297b-4a75-8775-8ba489da90d0\"\n",
    "\n",
    "batch_response = azure_openai_client.batches.retrieve(batch_id)\n",
    "status = batch_response.status\n",
    "print(f\"{time.time_ns()} Batch Id: {batch_id},  Status: {status}\")\n",
    "\n",
    "output_file_id = batch_response.output_file_id\n",
    "error_file_id = batch_response.error_file_id\n",
    "\n",
    "if output_file_id:\n",
    "    file_response = azure_openai_client.files.content(output_file_id)\n",
    "    raw_responses = file_response.text.strip().split('\\n')\n",
    "    for raw_response in raw_responses:\n",
    "        if not raw_response:\n",
    "            continue\n",
    "        json_response = json.loads(raw_response)\n",
    "        formatted_json = json.dumps(json_response, indent=2)\n",
    "        print(formatted_json)\n",
    "\n",
    "if error_file_id:\n",
    "    file_response = azure_openai_client.files.content(error_file_id)\n",
    "    raw_responses = file_response.text.strip().split('\\n')\n",
    "    for raw_response in raw_responses:\n",
    "        if not raw_response:\n",
    "            continue\n",
    "        json_response = json.loads(raw_response)\n",
    "        formatted_json = json.dumps(json_response, indent=2)\n",
    "        print(formatted_json)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6f65428fa60e6711",
   "metadata": {},
   "source": [
    "# valid schema\n",
    "batch_id = \"batch_ce73fcc8-5b70-4154-8aa0-be9961a0d147\"\n",
    "\n",
    "batch_response = azure_openai_client.batches.retrieve(batch_id)\n",
    "status = batch_response.status\n",
    "print(f\"{time.time_ns()} Batch Id: {batch_id},  Status: {status}\")\n",
    "\n",
    "output_file_id = batch_response.output_file_id\n",
    "error_file_id = batch_response.error_file_id\n",
    "\n",
    "if output_file_id:\n",
    "    file_response = azure_openai_client.files.content(output_file_id)\n",
    "    raw_responses = file_response.text.strip().split('\\n')\n",
    "\n",
    "    for raw_response in raw_responses:\n",
    "        json_response = json.loads(raw_response)\n",
    "        formatted_json = json.dumps(json_response, indent=2)\n",
    "        print(formatted_json)\n",
    "\n",
    "\n",
    "if error_file_id:\n",
    "    file_response = azure_openai_client.files.content(error_file_id)\n",
    "    raw_responses = file_response.text.strip().split('\\n')\n",
    "    for raw_response in raw_responses:\n",
    "        if not raw_response:\n",
    "            continue\n",
    "        json_response = json.loads(raw_response)\n",
    "        formatted_json = json.dumps(json_response, indent=2)\n",
    "        print(formatted_json)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "79df4bcccbbf40f5",
   "metadata": {},
   "source": [
    "### OpenAI\n",
    "\n",
    "Key Benefits:\n",
    "- 50% cost reduction compared to standard API\n",
    "- Guaranteed 24-hour turnaround time (most batches complete within 1 hour)\n",
    "- Does not consume tokens from standard per-model rate limits\n",
    "\n",
    "Quota & Limitations:\n",
    "- Batch size: Maximum 50,000 messages or 200 MB per batch\n",
    "- Output tokens: Unlimited\n",
    "- Request submissions: Unlimited\n",
    "- Results availability: Up to 24 hours\n",
    "- Data retention: Results expire after 24 hours\n",
    "\n",
    "Best Practices:\n",
    "- Monitor batch status regularly\n",
    "- Download results promptly before expiration\n",
    "- Consider batch size for optimal processing\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "93a08566d6f34209",
   "metadata": {},
   "source": [
    "message_requests = []\n",
    "timestamp = int(time.time())\n",
    "\n",
    "json_schema = type_to_response_format_param(Insight) # secure schema\n",
    "\n",
    "json_schema = {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"schema\": Insight.model_json_schema(),\n",
    "            \"name\": \"Insight\",\n",
    "            \"strict\": True,\n",
    "        },\n",
    "    }\n",
    "\n",
    "example_transcripts = [\n",
    "    open(os.path.join(transcripts_dir, \"14.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"15.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"16.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"17.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"18.txt\"), \"r\").read(),\n",
    "    open(os.path.join(transcripts_dir, \"19.txt\"), \"r\").read(),\n",
    "]\n",
    "\n",
    "for i, _example_transcript in enumerate(example_transcripts):\n",
    "    _result = {\n",
    "        \"custom_id\": f\"{i}--azure_openai--{timestamp}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": OPENAI_MODEL_BATCH,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\" + system_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"[Transcript]\\n {_example_transcript}\"\n",
    "                }\n",
    "            ],\n",
    "            \"response_format\": json_schema,\n",
    "\n",
    "        }\n",
    "    }\n",
    "    message_requests.append(_result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1ae411c9222d57cb",
   "metadata": {},
   "source": [
    "openai_batch_input_path = Path(\"./openai_batch_input.jsonl\")\n",
    "openai_batch_input_path.write_text(\"\\n\".join(json.dumps(message_request) for message_request in message_requests))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4eac2e7a77fbe255",
   "metadata": {},
   "source": [
    "openai_client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4913bf24134a53b0",
   "metadata": {},
   "source": [
    "file = openai_client.files.create(\n",
    "    file=open(openai_batch_input_path, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "print(file.model_dump_json(indent=2))\n",
    "openai_file_id = file.id"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "49b0be0fa882a319",
   "metadata": {},
   "source": [
    "openai_batch_response = openai_client.batches.create(\n",
    "    input_file_id=openai_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    ")\n",
    "openai_batch_id = openai_batch_response.id\n",
    "print(openai_batch_response.model_dump_json(indent=2))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e13d060cfae21c98",
   "metadata": {},
   "source": [
    "# valid schema\n",
    "openai_batch_id = \"batch_67def026bdfc81908b85f6b125273a67\"\n",
    "\n",
    "openai_batch_response = openai_client.batches.retrieve(openai_batch_id)\n",
    "status = openai_batch_response.status\n",
    "print(f\"{time.time_ns()} Batch Id: {openai_batch_id},  Status: {status}\")\n",
    "\n",
    "output_file_id = openai_batch_response.output_file_id\n",
    "error_file_id = openai_batch_response.error_file_id\n",
    "\n",
    "if output_file_id:\n",
    "    file_response = openai_client.files.content(output_file_id)\n",
    "    raw_responses = file_response.text.strip().split('\\n')\n",
    "\n",
    "    for raw_response in raw_responses:\n",
    "        json_response = json.loads(raw_response)\n",
    "        formatted_json = json.dumps(json_response, indent=2)\n",
    "        display(JSON(formatted_json, expanded=True))\n",
    "        print(get_llm_debug_openai(json_response['response']['body']['usage'], json_response['response']['body']['model']))\n",
    "        print()\n",
    "\n",
    "if error_file_id:\n",
    "    file_response = openai_client.files.content(error_file_id)\n",
    "    raw_responses = file_response.text.strip().split('\\n')\n",
    "    for raw_response in raw_responses:\n",
    "        if not raw_response:\n",
    "            continue\n",
    "        json_response = json.loads(raw_response)\n",
    "        formatted_json = json.dumps(json_response, indent=2)\n",
    "        print(formatted_json)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "769f9c46d83ac775",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "51856219a5a69d6a",
   "metadata": {},
   "source": [
    "# Level 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e3a50fc2f01bc4",
   "metadata": {},
   "source": [
    "## Schema optimisation\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7c25c9e1577eefc3",
   "metadata": {},
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model('gpt-4o')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "81dc27b873a7cb4e",
   "metadata": {},
   "source": [
    "#  JSON schema representation of nested Pydantic models\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "class InnerClass(BaseModel):\n",
    "    inner_attribute: str\n",
    "\n",
    "class OuterClass(BaseModel):\n",
    "    outer_attribute: str\n",
    "    inner_class: InnerClass\n",
    "\n",
    "schema = json.dumps(OuterClass.model_json_schema())\n",
    "print(schema)\n",
    "print(f\"schema tokens: {len(encoding.encode(str(schema)))}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d95fcdc947855e77",
   "metadata": {},
   "source": [
    "# Compare token size between JSON and YAML formats\n",
    "import yaml\n",
    "\n",
    "schema_yaml = yaml.dump(json.loads(schema), default_flow_style=False)\n",
    "\n",
    "print(schema_yaml)\n",
    "print(f\"schema_yaml tokens: {len(encoding.encode(str(schema_yaml)))}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a4c97cf4653c08d",
   "metadata": {},
   "source": [
    "### Flattened schema\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "bf3d4c95c8e50e91",
   "metadata": {},
   "source": [
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "# Flattened version of the nested schema that combines attributes into a single class\n",
    "class OuterClassFlat(BaseModel):\n",
    "    outer_attribute: str\n",
    "    inner_attribute: str\n",
    "\n",
    "schema = json.dumps(OuterClassFlat.model_json_schema())\n",
    "print(schema)\n",
    "print(f\"schema tokens: {len(encoding.encode(str(schema)))}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "84bbbe2d71c97fed",
   "metadata": {},
   "source": [
    "# Compare token size between JSON and YAML formats\n",
    "import yaml\n",
    "\n",
    "schema_yaml = yaml.dump(json.loads(schema), default_flow_style=False)\n",
    "\n",
    "print(schema_yaml)\n",
    "print(f\"schema_yaml tokens: {len(encoding.encode(str(schema_yaml)))}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "120f82c9d9ed2c7c",
   "metadata": {},
   "source": [
    "### Lightweight schema\n",
    " \n",
    "Techniques to reduce schema size:\n",
    "- Remove non-essential field metadata (titles, descriptions, examples)\n",
    "- Use shorter aliases for verbose attribute names\n",
    "- Minimize enum value repetition\n",
    "- Strip optional fields where possible\n",
    "- Use compact JSON format"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install -U git+https://github.com/nicholishen/tooldantic.git\n",
   "id": "91e15bf3c9d0e9db",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5612771bd7e3118",
   "metadata": {},
   "source": [
    "from tooldantic import GenericSchemaGenerator\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "class Color(Enum):\n",
    "    BLUE = \"blue\"\n",
    "    RED = \"red\"\n",
    "\n",
    "\n",
    "class InnerClass(BaseModel):\n",
    "    inner_attribute: str\n",
    "    color: Color\n",
    "\n",
    "\n",
    "class OuterClass(BaseModel):\n",
    "    outer_attribute: str\n",
    "    inner_class: InnerClass\n",
    "    inner_class2: InnerClass\n",
    "\n",
    "\n",
    "schema_td = json.dumps(OuterClass.model_json_schema(schema_generator=GenericSchemaGenerator))\n",
    "ref_schema = json.dumps(OuterClass.model_json_schema())\n",
    "\n",
    "print(ref_schema)\n",
    "print(schema_td)\n",
    "\n",
    "encoding = tiktoken.encoding_for_model('gpt-4o')\n",
    "print(len(encoding.encode(str(ref_schema))))\n",
    "print(len(encoding.encode(str(schema_td))))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "\n",
    "schema_yaml = yaml.dump(json.loads(schema_td), default_flow_style=False)\n",
    "\n",
    "print(schema_yaml)\n",
    "print(f\"schema_yaml tokens: {len(encoding.encode(str(schema_yaml)))}\")\n"
   ],
   "id": "86a7542507a003d7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
